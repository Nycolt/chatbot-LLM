````markdown
# MÃ³dulo LLM - ExtracciÃ³n de Intenciones

MÃ³dulo hÃ­brido para integraciÃ³n con **Ollama (local)** o **OpenRouter (cloud)** que convierte preguntas de usuarios en intenciones estructuradas y validadas.

## ğŸŒ Proveedores Soportados

- **Ollama** (por defecto) - Modelos locales: Phi-3, Llama, Mistral, etc.
- **OpenRouter** - Acceso a GPT-4, Claude, Gemini y mÃ¡s
- **Auto-detecciÃ³n** - Cambia automÃ¡ticamente segÃºn configuraciÃ³n

## ğŸ“ Estructura

```
src/
â”œâ”€â”€ config/
â”‚   â””â”€â”€ llm.config.js              # ConfiguraciÃ³n centralizada (Ollama + OpenRouter)
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ llm.service.js             # Servicio unificado con auto-detecciÃ³n
â”‚   â”œâ”€â”€ ollama.service.js          # Servicio Ollama (legacy)
â”‚   â””â”€â”€ llm/
â”‚       â”œâ”€â”€ OllamaAdapter.js       # Adapter para Ollama local
â”‚       â””â”€â”€ OpenRouterAdapter.js   # Adapter para OpenRouter cloud
â””â”€â”€ llm/
    â”œâ”€â”€ prompts/
    â”‚   â””â”€â”€ intent-extraction.v1.prompt.txt  # Prompt versionado
    â”œâ”€â”€ schemas/
    â”‚   â””â”€â”€ intent.schema.json     # JSON Schema de validaciÃ³n
    â”œâ”€â”€ validators/
    â”‚   â””â”€â”€ intent.validator.js    # Validador usando AJV
    â”œâ”€â”€ examples/
    â”‚   â””â”€â”€ intent-extraction.example.js  # Ejemplo completo de uso
    â””â”€â”€ index.js                   # Punto de entrada
```

## ğŸ¯ Contrato de IntenciÃ³n

```javascript
{
  "entity": "product",           // Solo "product" es vÃ¡lido
  "filters": {
    "brand": string | null,      // Marca del producto (opcional)
    "model": string | null       // Modelo del producto (opcional)
  },
  "fields": string[]             // Campos solicitados (ports, price, stock, etc.)
}
```

### Campos vÃ¡lidos
- `ports` - NÃºmero de puertos
- `price` - Precio
- `stock` - Disponibilidad
- `description` - DescripciÃ³n
- `brand` - Marca
- `model` - Modelo
- `name` - Nombre del producto
- `features` - CaracterÃ­sticas

## ğŸš€ Uso

### BÃ¡sico

```javascript
import { processUserQuestion } from './llm/examples/intent-extraction.example.js';

// Extraer y validar intenciÃ³n
const intent = await processUserQuestion('Â¿CuÃ¡ntos puertos tiene el forti32h?');

console.log(intent);
// {
//   "entity": "product",
//   "filters": {
//     "brand": null,
//     "model": "forti32h"
//   },
//   "fields": ["ports"]
// }
```

### Avanzado

```javascript
import llmService from './services/llm.service.js';
import intentValidator from './llm/validators/intent.validator.js';

async function processQuestion(userQuestion) {
  try {
    // Extraer intenciÃ³n (con reintentos automÃ¡ticos)
    // Usa Ollama o OpenRouter automÃ¡ticamente
    const rawIntent = await llmService.extractIntentWithRetry(userQuestion);
    
    // Validar con JSON Schema
    const validatedIntent = intentValidator.validateOrThrow(rawIntent);
    
    return validatedIntent;
  } catch (error) {
    console.error('Error:', error.message);
    throw error;
  }
}
```

## âš™ï¸ ConfiguraciÃ³n

### Variables de entorno (.env)

```bash
# SelecciÃ³n de proveedor: auto, ollama, openrouter
LLM_PROVIDER=auto

# Ollama (local)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=phi3
OLLAMA_TIMEOUT=3600000

# OpenRouter (cloud) - opcional
OPENROUTER_API_KEY=sk-or-v1-tu-clave
OPENROUTER_MODEL=anthropic/claude-3-sonnet
OPENROUTER_BASE_URL=https://openrouter.ai/api/v1
```

### LÃ³gica de Auto-DetecciÃ³n

1. **Si `OPENROUTER_API_KEY` existe** â†’ Usa OpenRouter
2. **Si Ollama disponible** (health check) â†’ Usa Ollama
3. **Si Ollama falla y hay API key** â†’ Fallback a OpenRouter
4. **Si nada funciona** â†’ Error claro

### Cambiar modelo LLM

Editar [src/config/llm.config.js](src/config/llm.config.js):

```javascript
export default {
  ollama: {
    model: 'llama2',  // Cambiar aquÃ­
    // ... resto de configuraciÃ³n
  }
};
```

## ğŸ“‹ Prerequisitos

### OpciÃ³n 1: Ollama (Local - Gratis)

1. **Ollama instalado y ejecutÃ¡ndose**
   ```bash
   # Instalar Ollama
   # https://ollama.ai/download
   
   # Descargar modelo Phi-3
   ollama pull phi3
   
   # Verificar que estÃ© corriendo
   ollama list
   ```

### OpciÃ³n 2: OpenRouter (Cloud - Pago)

1. **Obtener API key**
   - Registrarse en https://openrouter.ai
   - Obtener clave en https://openrouter.ai/keys
   - Configurar en `.env`:
   ```bash
   OPENROUTER_API_KEY=sk-or-v1-...
   ```

### Dependencias

2. **Dependencias instaladas**
   ```bash
   pnpm install
   ```

3. **Package.json configurado con ES modules**
   ```json
   {
     "type": "module"
   }
   ```

## ğŸ§ª Ejecutar ejemplos

```bash
node src/llm/examples/intent-extraction.example.js
```

EjecutarÃ¡ 4 ejemplos:
- âœ“ "Â¿CuÃ¡ntos puertos tiene el forti32h?"
- âœ“ "Â¿CuÃ¡l es el precio del Cisco 2960?"
- âœ“ "Dame informaciÃ³n del router Mikrotik RB3011"
- âœ“ "Â¿Hay stock de switches HP?"

## ğŸ—ï¸ Arquitectura

### SeparaciÃ³n de responsabilidades

1. **Config** ([llm.config.js](../src/config/llm.config.js))
   - ConfiguraciÃ³n centralizada
   - FÃ¡cil cambio de modelo/proveedor

2. **Prompts** ([*.prompt.txt](../src/llm/prompts/))
   - Versionados en archivos separados
   - No hardcodeados en el cÃ³digo
   - FÃ¡ciles de iterar y mejorar

3. **Service** ([ollama.service.js](../src/services/ollama.service.js))
   - ComunicaciÃ³n con Ollama
   - Manejo de errores y reintentos
   - Parseo de JSON
   - Cache de prompts

4. **Validator** ([intent.validator.js](../src/llm/validators/intent.validator.js))
   - ValidaciÃ³n estricta con AJV
   - Rechazo de respuestas invÃ¡lidas
   - Mensajes de error claros

5. **Schema** ([intent.schema.json](../src/llm/schemas/intent.schema.json))
   - Contrato formal de la intenciÃ³n
   - ValidaciÃ³n automÃ¡tica
   - DocumentaciÃ³n implÃ­cita

## âš¡ CaracterÃ­sticas

âœ… **Prompts versionados** - FÃ¡cil rollback y A/B testing  
âœ… **ValidaciÃ³n estricta** - JSON Schema asegura respuestas vÃ¡lidas  
âœ… **Reintentos automÃ¡ticos** - Backoff exponencial  
âœ… **Cache de prompts** - Mejor performance  
âœ… **Preparado para cambio de modelo** - ConfiguraciÃ³n centralizada  
âœ… **Manejo robusto de errores** - Mensajes claros y accionables  
âœ… **Sin lÃ³gica de BD** - Enfocado solo en extracciÃ³n de intenciÃ³n  

## ğŸ”§ PersonalizaciÃ³n

### Agregar nuevos campos

1. Editar [intent.schema.json](../src/llm/schemas/intent.schema.json):
```json
"fields": {
  "items": {
    "enum": ["ports", "price", "stock", "nuevoCampo"]
  }
}
```

2. Actualizar [intent-extraction.v1.prompt.txt](../src/llm/prompts/intent-extraction.v1.prompt.txt) con ejemplos

3. Recargar schema:
```javascript
intentValidator.reloadSchema();
```

### Versionar prompts

Crear nuevo archivo `intent-extraction.v2.prompt.txt` y actualizar config:

```javascript
// llm.config.js
paths: {
  intentPromptVersion: 'v2'  // Cambiar aquÃ­
}
```

## ğŸ“Š Ejemplo de flujo completo

```
Usuario: "Â¿CuÃ¡ntos puertos tiene el forti32h?"
   â†“
[Ollama Service] Carga prompt + pregunta
   â†“
[Ollama/Phi-3] Genera respuesta JSON
   â†“
{
  "entity": "product",
  "filters": { "brand": null, "model": "forti32h" },
  "fields": ["ports"]
}
   â†“
[Intent Validator] Valida contra schema
   â†“
âœ“ IntenciÃ³n validada lista para usar
```

## ğŸš¨ Manejo de errores

El mÃ³dulo rechaza:
- âŒ Respuestas no-JSON
- âŒ Campos extra no permitidos
- âŒ Entity diferente de "product"
- âŒ Fields con valores invÃ¡lidos
- âŒ Filtros mal formados
- âŒ Timeouts del modelo

## ğŸ“ Notas importantes

- **No incluye lÃ³gica de base de datos** - Solo extracciÃ³n de intenciÃ³n
- **No genera SQL** - La intenciÃ³n debe ser procesada por otro mÃ³dulo
- **No responde preguntas** - Solo estructura la pregunta

## ğŸ” Seguridad

- âœ… ValidaciÃ³n estricta de entrada (JSON Schema)
- âœ… No se permiten campos adicionales (additionalProperties: false)
- âœ… Enum limita valores posibles
- âœ… Sin inyecciÃ³n de prompts (prompt separado del input)

````
