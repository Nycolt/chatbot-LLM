````markdown
# ğŸš€ GuÃ­a RÃ¡pida - MÃ³dulo LLM HÃ­brido

## SelecciÃ³n de Proveedor

El sistema **detecta automÃ¡ticamente** quÃ© proveedor usar:

- ğŸ  **Ollama** (local, gratis) - Por defecto
- â˜ï¸ **OpenRouter** (cloud, pago) - Si configuras API key

## InstalaciÃ³n y Setup

### OpciÃ³n A: Ollama (Recomendado para desarrollo)

#### 1. Instalar Ollama
```bash
# Windows: Descargar desde https://ollama.ai/download
# Linux/Mac:
curl -fsSL https://ollama.ai/install.sh | sh
```

### 2. Descargar modelo Phi-3
```bash
ollama pull phi3
```

### 3. Verificar instalaciÃ³n
```bash
ollama list
# Debe aparecer phi3
```

### 4. Iniciar Ollama (si no estÃ¡ corriendo)
```bash
ollama serve
```

### OpciÃ³n B: OpenRouter (Recomendado para producciÃ³n)

#### 1. Obtener API Key
```bash
# Registrarse en https://openrouter.ai
# Obtener clave en https://openrouter.ai/keys
```

#### 2. Configurar en .env
```bash
OPENROUTER_API_KEY=sk-or-v1-tu-clave-aqui
OPENROUTER_MODEL=anthropic/claude-3-sonnet
```

#### 3. El sistema cambiarÃ¡ automÃ¡ticamente a OpenRouter
```bash
# Logs al iniciar:
# ğŸ¤– LLM Provider: OpenRouter (anthropic/claude-3-sonnet) - OPENROUTER_API_KEY configured
```

## Uso RÃ¡pido

### OpciÃ³n 1: Uso simple
```javascript
import llm from './src/llm/index.js';

const intent = await llm.extractIntent('Â¿CuÃ¡ntos puertos tiene el forti32h?');
console.log(intent);
// {
//   "entity": "product",
//   "filters": { "brand": null, "model": "forti32h" },
//   "fields": ["ports"]
// }
```

### OpciÃ³n 2: Uso seguro (sin excepciones)
```javascript
import llm from './src/llm/index.js';

const result = await llm.extractIntentSafe('Â¿CuÃ¡ntos puertos tiene el forti32h?');

if (result.valid) {
  console.log('IntenciÃ³n:', result.data);
} else {
  console.log('Errores:', result.errors);
}
```

## Probar el mÃ³dulo

```bash
# Ejecutar ejemplos
node src/llm/examples/intent-extraction.example.js
```

## IntegraciÃ³n en tu controlador

```javascript
// En tu controlador de productos
import llm from '../llm/index.js';

async function handleUserQuestion(req, res) {
  try {
    const { question } = req.body;
    
    // Extraer intenciÃ³n
    const intent = await llm.extractIntent(question);
    
    // intent.entity -> "product"
    // intent.filters.brand -> "cisco" o null
    // intent.filters.model -> "2960" o null
    // intent.fields -> ["price", "stock"]
    
    // AquÃ­ usarÃ­as la intenciÃ³n para consultar tu BD
    // const products = await productService.findByIntent(intent);
    
    res.json({ intent });
  } catch (error) {
    res.status(400).json({ error: error.message });
  }
}
```

## Troubleshooting

### Error: "fetch failed" o "ECONNREFUSED"
- âœ“ Verifica que Ollama estÃ© corriendo: `ollama list`
- âœ“ Inicia Ollama si es necesario: `ollama serve`
- âœ“ O configura OpenRouter como fallback

### Error: "model not found"
- âœ“ Descarga el modelo: `ollama pull phi3`
- âœ“ O cambia a OpenRouter configurando API key

### Error: "No LLM provider available"
- âœ“ Inicia Ollama: `ollama serve`
- âœ“ O configura `OPENROUTER_API_KEY` en `.env`

### Respuestas invÃ¡lidas del modelo
- âœ“ El mÃ³dulo reintenta automÃ¡ticamente 3 veces
- âœ“ Ajusta la temperatura en `src/config/llm.config.js`

### Timeout
- âœ“ Aumenta `OLLAMA_TIMEOUT` en `.env`
- âœ“ Verifica recursos del sistema (RAM, CPU)

## Ejemplos de preguntas soportadas

âœ… "Â¿CuÃ¡ntos puertos tiene el forti32h?"  
âœ… "Â¿CuÃ¡l es el precio del Cisco 2960?"  
âœ… "Dame informaciÃ³n del router Mikrotik RB3011"  
âœ… "Â¿Hay stock de switches HP?"  
âœ… "CaracterÃ­sticas del switch Cisco"  
âœ… "Â¿CuÃ¡nto cuesta el producto X?"  

## Variables de entorno (.env)

```bash
# Proveedor (auto detecta si no se especifica)
LLM_PROVIDER=auto

# Ollama (local)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=phi3
OLLAMA_TIMEOUT=3600000

# OpenRouter (cloud)
OPENROUTER_API_KEY=sk-or-v1-...  # Descomenta para activar
OPENROUTER_MODEL=anthropic/claude-3-sonnet
```

## Cambiar de modelo

### Ollama
En `.env`:
```bash
OLLAMA_MODEL=llama2
```

Descargar el nuevo modelo:
```bash
ollama pull llama2
```

### OpenRouter
En `.env`:
```bash
OPENROUTER_MODEL=openai/gpt-4-turbo
# o
OPENROUTER_MODEL=meta-llama/llama-3-70b
```

Ver modelos disponibles: https://openrouter.ai/models

## PrÃ³ximos pasos

1. âœ… El mÃ³dulo extrae la intenciÃ³n
2. ğŸ“ **TÃº implementas**: Usar la intenciÃ³n para consultar la BD
3. ğŸ“ **TÃº implementas**: Formatear la respuesta para el usuario

El mÃ³dulo NO:
- âŒ No genera SQL
- âŒ No consulta la base de datos
- âŒ No formatea respuestas para el usuario

Solo convierte pregunta â†’ intenciÃ³n estructurada y validada.

````
